\documentclass[a4paper]{article}
\usepackage{float}
\usepackage{parskip}
% \setcounter{tocdepth}{2}

\def\nterm {Spring}
\def\nyear {2025}
\def\ncourse {Machine Learning}

\input{../header}

\begin{document}
\maketitle

\tableofcontents

\section{Probability and statistical inference}

\subsection{Basic probability theory}
\begin{defi}[Convergence of random vairables]
Let $\left\{ X_n \right\}_{n=1}^\infty$ be a sequence of random
variables and $X$ be another random variable. Let
$F_n$ be the CDF of $X_n$ for each $n \in \N$ and $F$
be the CDF of $X$.
\begin{enumerate}
  \item $X_n$ converges to $X$ \textbf{in probability}
  and write $X_n \convProb X$ if for arbitrary
  $\epsilon > 0$,
  \[
  \Pr \left[ \abs{X_n - X} > \epsilon \right] \to 0
  \]
  as $n \to \infty$.

  \item $X_n$ converges to $X$ \textbf{in distribution} and
  write $X_n \convDist X$ if
  \[
  \lim_{n \to \infty} F_n(t) = F(t)
  \]
  for all $t$ where $F$ is continuous.

  \item $X_n$ converges to $X$ in $L^p$ if
  \[
  \E \left[ \abs{X_n - X}^p \right] \to 0
  \]
  as $n \to \infty$. In particular, say $X_n$ converges to
  $X$ in \textbf{quadratic mean} and write $X_n \convQM X$
  if $X_n$ converges to $X$ in $L^2$.

  \item $X_n$ converges to $X$ \textbf{almost surely}
  and write $X_n \convAS X$ if
  \[
  \Pr \left[ \lim_{n \to \infty} X_n =  X \right] = 1.
  \]
\end{enumerate}
\end{defi}

\begin{thm}
The following implication holds:
\begin{enumerate}
  \item If $X_n$ converges to $X$ almost surely,
  then $X_n$ converges to $X$ in probability.

  \item If $X_n$ converges to $X$ in $L^p$, then
  $X_n$ converges to $X$ in probability.
\end{enumerate}
\end{thm}

\begin{proof}

\begin{enumerate}
  \item If $X_n$ converges to $X$ almost surely,
  the set of points 
  \[
  O = \left\{ \omega : \lim_{n \to \infty}
  X_n(\omega) \neq X(\omega) \right\}
  \]
  has measure zero.
  Now fix $\epsilon > 0$ and consider the sequence of
  sets
  \[
  A_n = \bigcup_{m = n}^\infty \left\{ \abs{X_m - X} > \epsilon \right\}.
  \]
  Note that $A_n \supset A_{n+1}$ for each $n \in \N$ and
  let $A_\infty = \bigcap_{n=1}^\infty A_n$.
  Now show $\Pr[A_\infty] = 0$. If $\omega \notin O$, then
  $\lim_{n \to \infty} X_n (\omega) = X(\omega)$ and thus
  $\abs{X_n(\omega) - X(\omega)} < \epsilon$ for some $n \in \N$.
  Therefore, $\omega \notin A_\infty$.
  It follows that $A_\infty \subset O$ and $\Pr[A_\infty] = 0$.

  By monotone
  continuity, we have $\lim_{n \to \infty} \Pr[A_n] =
  \Pr[A_\infty]$. It follows that
  \[
  \Pr \left[ \abs{X_n - X} > \epsilon \right]
  \leq \Pr \left[ A_n \right] \to 0
  \]
  as $n \to \infty$. This completes the proof.

  \item From Chebyshev's inequality, we have
  \[
  \Pr \left[ \abs{X - X_n} > \epsilon \right] \leq
  \frac{1}{\epsilon^p} \E [\abs{X - X_n}^p].
  \]
  The claim follows directly.
\end{enumerate}

\end{proof}

\begin{thm}[Central Limit Theorem]
  Let $X_1, \dots, X_n$ be i.i.d. with mean $\mu$ and variance
  $\sigma^2$. Let $S_n = \frac{1}{n} \sum_{i=1}^n X_i$.
  Then
  \[
  Z_n = \frac{S_n - \mu}{\sqrt{\var S_n}}
  = \frac{\sqrt{n} \left( S_n - \mu \right)}{\sigma}
  \convDist Z,
  \]
  where $Z \sim \ncal(0, 1)$ and $\convDist$ represents 
  convergence in distribution. In other words,
  \[
  \lim_{n \to \infty} \Pr[Z_n < z] = \Phi(z)
  = \int_{-\infty}^z \frac{1}{\sqrt{2\pi}} e^{- \frac{x^2}{2}}
  \; dx.
  \]
  Also write $Z_n \approx \ncal(0, 1)$.
\end{thm}

\subsection{Statistical inference}

\begin{defi}
  Let $X_1, \dots, X_n$ be $n$ i.i.d. data points observed
  from some distribution $F$ with respect to parameter
  $\theta$. A point estimator $\hat{\theta}_n$
  of the parameter $\theta$ is some function of
  $X_1, \dots, X_n$:
  \[
  \hat{\theta}_n = g(X_1, \dots, X_n).
  \]
  The bias of an estimator is defined as
  \[
  \bias (\hat{\theta}_n)
  = \E_\theta [\hat{\theta}_n] - \theta.
  \]
  The mean squared error is defined as
  \[
  \mse = \E_\theta (\hat{\theta}_n - \theta)^2.
  \]
\end{defi}

\begin{defi}[Consistent point estimator]
  A point estimator $\hat\theta_n$ of a parameter
  $\theta$ is \textbf{consistent} if $\hat{\theta}_n
  \convProb \theta$.
\end{defi}

Next we an important relation between bias, variance,
and MSE. This is a more rigorous way to express the
\textbf{bias-variance tradeoff} of point estimators.

\begin{thm}
The MSE can be written as
\[
\mse = \bias^2 (\hat{\theta}_n) + \var_\theta (\hat{\theta}_n).
\]
\end{thm}

\begin{proof}
  Let $\bar{\theta}_n = \E_\theta (\hat{\theta}_n)$.
  Then we have
  \[
  \begin{aligned}
    \E_\theta(\theta - \hat{\theta}_n)^2
    &= \E_\theta(\theta - \bar{\theta}_n + \bar{\theta}_n
    - \hat{\theta}_n)^2 \\
    &= \E_\theta(\theta - \bar{\theta}_n)^2
    - 2 (\theta - \bar{\theta}_n) \E_\theta (\bar{\theta}_n -
    \hat{\theta}_n)
    + \E_\theta (\bar{\theta}_n - \theta)^2 \\
    &= (\theta - \bar{\theta}_n)^2
    + \E_\theta (\bar{\theta}_n - \theta)^2 \\
    &= \bias^2(\hat{\theta}_n) + \var_\theta (\hat{\theta}_n),
  \end{aligned}
  \]
  where we have used the fact that
  $\E_\theta (\bar{\theta}_n -
  \hat{\theta}_n) = \bar{\theta}_n - E_\theta(\hat{\theta}_n)
  = \bar{\theta}_n - \bar{\theta}_n = 0$.
\end{proof}

Below is the definition of a confidence set/interval.

\begin{defi}
  A $1 - \alpha$ interval for a parameter $\theta$ is
  an interval $C_n = (a, b)$ where
  $a = a(X_1, \dots, X_n)$ and $b = b(X_1, \dots, X_n)$
  are functions of data such that
  \[
  \Pr_\theta[\theta \in C_n] \geq 1 - \alpha
  \text{ for all $\theta \in \Theta$. }
  \]
  In other word, $(a, b)$ traps $\theta$ with probablity
  $1 - \alpha$.
\end{defi}

\textbf{Warning!} In the above definition,
$C_n$ is random and $\theta$ is fixed.

\section{Learning theory}

\subsection{PAC learning}

PAC learning is short for Probably Approxinate Correct
learning, and the setting of PAC learning is as follows:
\begin{itemize}
  \item We have data $x \in \R^d$ and label $y \in \left\{ -1,
  +1 \right\}$.
  \item We collect features $x_1, \dots, x_n$ i.i.d. from
  some distribution $D$. Note that we make no assumption on
  the distribution $D$ of the features.
  \item We collect corresponding labels $y_1, \dots, y_n$.
  \item Assume there exists some true classifier $h^\star$.
  \item Let $\hcal$ be the set of all hypotheses.
\end{itemize}
The goal of PAC learning is $(\epsilon, \delta)$-PAC,
which is defined as follows.
\begin{defi}
  An $(\epsilon, \delta)$-PAC learning algorithm
  refers to an algorithm that
  picks an hypothesis $h \in \hcal$
  after observing training data
  $\left\{ x_i, y_i \right\}_{i=1}^n$,
  where the hypothesis $h \in \hcal$ satisfies
  \[
  \err_D (\hat{h}) =
  \Pr_{x \sim D} \left[ \hat{h}(x) \neq h^\star(x) \right]
  < \epsilon.
  \]
  with probability at least $1 - \delta$, where the probability
  is with respect to the randomness of the training set.
\end{defi}
Putting it more concretely, suppose $g$ is a point estimator
(the algorithm), then we want
\[
\Pr_{x_i \sim D} \left[ \err_D (g(x_1, \dots, x_n)) < \epsilon
\right] \geq 1 - \delta.
\]
Through the $\epsilon$-$\delta$ definition of limit,
it is not hard to
notice the similarity between this definition
and \textbf{convergence in probability}.
As we observe more and more data ($n \to \infty$),
we want the hypothesis
produce by the algorithm to converge to the true classifier
\textbf{in probability}. This is exactly the definition
of a \textbf{consistent} point estimator in the previous
section.

We also say the algorithm is a PAC learner if it uses
$n$ samples and the running time is at most $\poly
\left( d, \frac{1}{\epsilon}, \log \frac{1}{\delta},
\bits(h^\star) \right)$, but in this section we do not
focus on the runtime of the algorithm.

There are two types of PAC learning -- realizable PAC learning,
in which case $h^\star \in \hcal$, and agnostic PAC learning,
in which case $h^\star \notin \hcal$. We first discuss
realizable PAC learning, and we will find out agnostic PAC
learning is a more general setting but than PAC learning
but a natural extension.

\subsubsection{Realizable learning}

For realizable PAC learning, we present a simple algorithm:
\begin{algorithm}[Consistent Learner]
Pick any $\hat{h} \in \hcal$ such that $h(x_i) = y_i$
for all $1 \leq i \leq n$.
\end{algorithm}
Now we analyze this algorithm in terms of the sample size needed
to produce a desired hypothesis.

\begin{thm}
  Over the dataset of $n$ i.i.d. samples,
  the consistent learning algorithm produces $\hat{h}$ such
  that $\err_D(\hat{h}) > \epsilon$ with probability
  at most $\abs{\hcal} e^{-n\epsilon}$.
\end{thm}
\begin{proof}
  Suppose $h \in \hcal$ is such that
  $\err_D(h) = \Pr_{x \sim D} [h(x) \neq h^\star(x)] > \epsilon$.
  For such an $h$ and some data $x_i$, we have
  \[
  \Pr_{x_i \sim D} [h(x_i) = y_i = h^\star(x_i)] < 1 - \epsilon.
  \]
  Since $\left\{ x_i, y_i \right\}_{i=1}^n$ are i.i.d., we have
  \[
  \Pr_{x_{1:n} \sim D}[h(x_i) = h^\star(x_i) \text{ for all
  $1 \leq i \leq n$}] < (1 - \epsilon)^n.
  \]
  Note that our consistent learner do not make any mistake
  on the training set $\left\{ x_i, y_i \right\}_{i=1}^n$
  \[
  \Pr_{x_{1 : n} \sim D} [h = \hat{h}] < (1 - \epsilon)^n.
  \]
  It follows that
  \[
  \begin{aligned}
    \Pr_{x_{1 : n} \sim D} [\err_D (\hat{h}) > \epsilon]
    & \leq \sum_{h \in \hcal: \; \err_D(h) > \epsilon}
    \Pr[\hat{h} = h]  \\
    & \leq \abs{\hcal} (1 - \epsilon)^n \\
    & \leq \abs{\hcal} e^{-n\epsilon},
  \end{aligned}
  \]
  where in the last step we used the inequality
  $(1 - u)^n \leq e^{-nu}$. This completes the proof.
\end{proof}

\begin{cor}
  If we want $\Pr_{x_{1 : n} \sim D} [\err_D (\hat{h}) > \epsilon]
  \leq \delta$, we must have
  \[
  n \geq \frac{\log \left( \abs{\hcal} / \delta \right)}{\epsilon}.
  \]
\end{cor}
Technically speaking the implication should be the other direction,
but this bound for sample size $n$
\textbf{gurantees} $(\epsilon, \delta)$-PAC.

To illustrate how we should think of $\abs{\hcal}$,
we present an example.
\begin{eg}
Consider the binary half-spaces hypotheses:
\[
\hcal = \left\{ h_w(x) = \sign(\braket{w}{x}) :
w_i \in \left\{ -1, 1 \right\} \right\}.
\]
In this case $\abs{\hcal} = 2^d$, so $\log \abs{\hcal} =
\Theta(d)$.

We should always think of $\abs{\hcal}$
as exponential with respect to the dimension, so
$\log \abs{\hcal}$ is linear with respect to dimension.
\end{eg}

Now we move on to the setting of agnostic leraning.

\subsubsection{Agnostic leraning}

In agnostic learning, again we collect features $x_1, \dots,
x_n \sim D$ i.i.d., and labels $y_1, \dots, y_n$. This forms a
data set $S_n = \left\{ x_i, y_i \right\}_{i=1}^n$. The
goal is to use this dataset $S_n$ to produce an hypothesis
$\hat{h}$ such that
\[
\reg_{D, \hcal} (\hat{h}) =
\err_D (\hat{h}) - \min_{h \in \hcal} \err_D (h) < \epsilon
\]
with probability at least $1 - \delta$, where the probability
is with respect to the randomness of the dataset $S_n$.

For this setting, we present an algorithm called the empirical
loss minimizer (ERM).
\begin{algorithm}[empirical loss minimizer]
  Choose $\hat{h} \in \hcal$ by minimizing the $\left\{ 0,1
   \right\}$ loss:
  \[
  \hat{h} = \argmin_{h \in \hcal} \sum_{i=1}^n \ind \left\{
    h(x_i) = y_i
   \right\}.
  \]
  In a more general setting, suppose $\ell(\cdot, \cdot)$
  be any loss function bounded in $[0, 1]$, choose
  $\hat{h} \in \hcal$ by minimizing the loss:
  \[
  \hat{h} = \argmin_{h \in \hcal} \sum_{i=1}^n \ell(h(x_i),
  y_i).
  \]
\end{algorithm}

We next prove a similar relation between the sample
size and the performance of the hypothesis, evaluated
in terms of risk. We first present the definition
of risk.

\begin{defi}[risk]
  The \textbf{risk} of a hypothesis $h$ is defined as
  \[
  \risk_D (h) = \E_{x \sim D} [ \ell(h(x), y) ].
  \]
\end{defi}

\begin{thm}
  Over the dataset of $n$ i.i.d. samples, the ERM algorithm
  produces $\hat{h}$ such that
  \[
  \reg_{D, \hcal} (\hat{h})
  = \risk_D (\hat{h}) - \min_{h \in \hcal}
  \risk_D (h)
  \leq 2 \epsilon.
  \]
  with probability at least $1 - 2 \abs{\hcal} e^{- 2
  n \epsilon^2}$.
\end{thm}

\begin{proof}
{
  \newcommand{\hopt}{h^{\text{opt}}}
  First we define
  \[
    \hopt = \argmin_{h \in \hcal} \risk_D (h).
  \]
  Note that $\reg_{D, \hcal}(\hopt) = 0$. Define also the
  estimated risk of hypothesis $h$ for data set $S$:
  \[
  \hat{\risk}_S (h) := \frac{1}{n} \sum_{i=1}^n \ell(h(x_i),
  y_i).
  \]
  Note that $\hat{h} = \argmin_{h \in \hcal}
  \hat{\risk}_S (h)$ and $\E[\hat{\risk}_S (h)] =
  \risk_D(h)$, where the expected value is with respect to
  the randomness of the dataset. Note that
  \begin{equation*}
    \begin{aligned}
      \reg_{D, \hcal} (\hat{h})
      &= \risk_D (\hat{h}) - \risk_D (\hopt) \\
      &= (\risk_D (\hat{h}) - \hat{\risk}_S (\hat{h}))
      + (\hat{\risk}_S (\hat{h}) - \hat{\risk}_S (\hopt))
      - (\risk_D (\hopt) - \hat{\risk}_S (\hopt))  \\
      &\leq (\risk_D (\hat{h}) - \hat{\risk}_S (\hat{h}))
      - (\risk_D (\hopt) - \hat{\risk}_S (\hopt)),
    \end{aligned}
    \tag{$*$}
    \label{reg}
  \end{equation*}
  where the inequality is because our algorithm
  always gives
  $\hat{\risk}_S (\hat{h}) - \hat{\risk}_S (\hopt) \leq 0$.

  For any $h \in \hcal$, we have
  \[
  \begin{aligned}
    \hat{\risk}_S (h) - \risk_D(h)
    &= \frac{1}{n} \sum_{i=1}^n \ell(h(x_i), y_i)
    - \risk_D (h) \\
    &= \frac{1}{n} \sum_{i=1}^n \left(
      \ell(h(x_i), y_i) - \E_{x, y \sim D}
      [\ell(h(x), y)]
     \right).
  \end{aligned}
  \]
  Define now random variable $Z_i := \ell(h(x_i), y_i) -
  \E_{x, y \sim D} [\ell(h(x), y)]$. Notice that $Z_i$
  i.i.d., $\E[Z_i] = 0$, and $\abs{Z_i} \leq 1$. Therefore,
  we can utilize the \textbf{Hoeffding bound} to get
  \[
  \Pr \left[ \frac{1}{n} \sum_{i=1}^n Z_i > \epsilon \right]
  \leq e^{- 2 n \epsilon^2} 
  \quad \text{ and } \quad
  \Pr \left[ \frac{1}{n} \sum_{i=1}^n Z_i < -\epsilon \right]
  \leq e^{- 2 n \epsilon^2}
  \]
  This implies that for any fixed $h \in \hcal$,
  \[
  \Pr \left[ \abs{\hat{\risk}_S(h) - \risk_D(h)} > \epsilon
  \right] \leq 2 e^{-2 n \epsilon^2}.
  \]
  It follows from the union bound that
  \[
  \Pr \left[ \exists h \in \hcal: \;
  \abs{\hat{\risk}_S(h) - \risk_D(h)} > \epsilon \right]
  \leq 2 \abs{\hcal} e^{- 2 n \epsilon^2}.
  \]
  Notice that if
  $\abs{\hat{\risk}_S (h) - \risk_D (h)} \leq \epsilon$
  for all $h \in \hcal$, we must have $\reg_{D, \hcal}
  (\hat{h}) \leq 2 \epsilon$ by inequality \eqref{reg}.
  above. Therefore,
  $\reg_{D, \hcal} (\hat{h}) \leq 2 \epsilon$ with probability at least
  $1 - 2 \abs{\hcal} e^{- 2 n \epsilon^2}$.
}
\end{proof}

\begin{cor}
Let $\delta > 0$ be given. Then,
setting
$n = \frac{1}{2\epsilon^2} \log \left( 2 \abs{\hcal} /
\delta \right)$, we obtain that $\reg_{D, \hcal} (\hat{h})
\leq 2\epsilon$ with probability at least $1 - \delta$.
Therefore, if we want $\reg_{D, \hcal}
(\hat{h}) \leq \epsilon$ with probability at least $1 -
\delta$, we must have
\[
  n \geq \frac{2 \log
  \left( 2 \abs{\hcal} / \delta \right)}
  {\epsilon^2}.
\]
\end{cor}

As a comparison between realizable learning and agnostic
learning, we can see the sample size needed to
achieve the same $(\epsilon, \delta)$-PAC bound is
\[
n = \Theta \left( \frac{\log \left( \abs{\hcal} / \delta
\right)}{\epsilon} \right)
\]
for realizable learning, and
\[
n = \Theta \left( \frac{\log(\abs{\hcal} / \delta)}{\epsilon^2}
\right)
\]
for agnostic learning. Since $\epsilon < 1$,
we can deduce that agnostic learning needs
\textbf{more sample} for the estimator to achieve the
same $(\epsilon, \delta)$-PAC bound.



\begin{thm}[Hoeffding inequality]
Let $X_1, \dots, X_n$ be independent random variables
such that $a_i \leq X_i \leq b_i$ almost surely.
Let 
\[
S = X_1 + \dots + X_n.
\]
Then for all $t > 0$, we have 
\[
\begin{aligned}
\Pr [S - \E[S] \geq t] &\leq \exp \left[ \frac{2t^2}
{\sumi^n (b_i - a_i)^2} \right], \\
\Pr [\abs{S - \E[s]} \geq t] &\leq 2\exp \left[ \frac{2t^2}
{\sumi^n (b_i - a_i)^2} \right].
\end{aligned}
\]
\end{thm}
  

\subsection{Infinite hypotheses space and VC dimension}

In last section, we discussed PAC learning with finite
hypotheses space $\hcal$. However, we often need
to deal with situations where the hypotheses space
is infinite. Consider the following example.

\begin{eg}
  Consider the binary half space hypotheses:
  \[
  \hcal = \left\{ h_w(x) =
  \sign(\braket{w}{x}) : w \in \R^d \right\}.
  \]
  Then, $\abs{\hcal} = \infty$ since each
  $w \in \R^d$ gives a different decision boundary.
\end{eg}

However, on some fixed dataset $S$, not all of the hypotheses
in the hypotheses space $\hcal$ gives a different prediction.
This leads to the following definition and theorem, which
charaterize the ``effective size'' of the hypotheses
space.

\begin{defi}
  Let $S$ be a dataset with $n$ points
  $\left\{ x_1, \dots, x_n \right\}$. Define
  $\abs{\hcal(S)}$ as the number of distinct labeling
  $\left\{ (x_i, y_i) \right\}_{i=1}^n$, where each
  $y_i = h(x_i)$ for a fixed $h \in \hcal$.
\end{defi}

With this definition, we can derive a similar confidence
bound for our learned hypotheses.

\begin{thm}
  Define the effective size
  \[
  \hcal [n] = \sup_{x_1, \dots, x_n}
  \abs{\hcal(\left\{ x_1, \dots, x_n \right\})},
  \]
  and let $\hat{h}_n$ denote the consistency learner
  from $n$ examples. Then we have
  \[
  \Pr_{x \sim D} [\err_D(\hat{h}) \geq \epsilon] \leq \delta,
  \]
  where
  \[
  \epsilon = O \left(
    \frac{\log \hcal[n] + \log (1 / \delta)}{n}
   \right).
  \]
\end{thm}

\begin{eg}
  Consider the threshold hypotheses space:
  \[
  \hcal = \left\{ h_w(x) = 2 \cdot
  \ind \left\{ x > w \right\} - 1 : w \in \R \right\}.
  \]
  Then it is easy to see that $\hcal[n] = n$.
  It follows that
  \[
  \epsilon = O\left(
    \frac{\log \hcal[n] + \log (1 / \delta)}{n}
   \right) = O\left(
    \frac{\log n + \log (1 / \delta)}{n}
   \right),
  \]
  which approaches to $0$ as $n \to \infty$.
\end{eg}

Another way to characterize the ``effective size'' of the
hypotheses space is \textbf{VC dimension}.

\begin{defi}[VC dimension]
  For a hypotheses space $\hcal$, we say
  $\hcal$ \textbf{shatters} a dataset
  $\left\{ x_1, \dots, x_n \right\}$ if
  for any choice of labels
  $\left\{ -1, 1 \right\}^n$, there exists
  $h \in \hcal$ such that $h(x_i) = y_i$
  for all $1 \leq i \leq n$.

  The \textbf{VC dimension} of a hypotheses
  space $\hcal$ is defined
  as the largest $n$ such that there exists a dataset $S$
  of size $n$ such that $\hcal$ shatters $S$.
  Denote this as $\VC(\hcal) = n$.
\end{defi}

The VC dimension of a hypotheses space $\hcal$ is
related to $\hcal[n]$ by the following lemma by
Sauer:

\begin{thm}[Sauer's Lemma]
  For any hypotheses class $\hcal$, we have
  \[
  \log \hcal[n] \leq O(\VC(\hcal) \cdot \log n).
  \]
\end{thm}

Recalling our bound for realizable learning, we get
$(\epsilon, \delta)$-PAC with
\[
\epsilon = O \left( \frac{\log \hcal[n] + \log (1 / \delta)}{n} \right)
= O \left( \frac{\VC(\hcal) \cdot \log n + \log (1 / \delta)}{n} \right).
\]

\section{Probability distributions}

\subsection{Binary variables}

\begin{defi}[binomial distribution]
The binomial distribution is given by 
\[
\binomial(m \mid N, \mu) 
= \binom{N}{m} \mu^{m} (1 - \mu)^{N - m},
\]
where 
\[
\binom{N}{m} = \frac{N!}{(N - m)! m!} 
\]
is the binomial coefficient.
\end{defi}

\begin{defi}[beta distribution]
The beta distribution is given by 
\[
\betaD(\mu \mid a, b) 
= \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)}
\mu^{a - 1} (1 - \mu)^{b - 1},
\]
where $\Gamma(x)$ is the Gamma function given by 
\[
\Gamma(x) = \int_0^\infty t^{x - 1} e^{-t} \d t.
\]
\end{defi}
The mean and variance of the beta distribution is given 
by 
\[
\begin{aligned}
\E[\mu] = \frac{a}{a + b}
\quad \text{ and } \quad
\var[\mu] = \frac{ab}{(a + b)^2 (a + b + 1)}.
\end{aligned}
\]

\begin{defi}[conjugate distribution]
In Bayesian probability theory, if, given a likelihood 
function $p(x \mid \theta)$, the posterior distribution 
$p(\theta \mid x)$ is in the same probability distribution 
family as the prior probability distribution 
$p(\theta)$, the prior and posterior are then called 
\textbf{conjugate distributions} with respect to that 
likelihood function and the prior is called a \textbf{conjugate 
prior} for the likelihood function $p(x \mid \theta)$. 
\end{defi}

It is easy to verify that the beta distribution $\betaD
(\mu \mid a, b)$ is  
the \textbf{conjugate prior} of the binomial distribution
$\binomial(m \mid N, \mu)$.

\subsection{Multinomial vairables}

\begin{defi}[multinomial distribution]
The multinomial distribution is given by 
\[
\multinomial(m_1, \dots, m_K \mid \mu, N)
= \binom{N}{m_1 \; m_2 \; \cdots \; m_K} 
\prod_{k=1}^K \mu_k^{m_k},
\]
where $\sum_{k=1}^K m_k = N$, 
$\mu = (\mu_1, \dots, \mu_K)$ is such that 
$\sum_{k=1}^K \mu_k = 1$, and 
\[
\binom{N}{m_1 \; m_2 \; \cdots \; m_K}
= \frac{N!}{m_1! \; m_2! \; \cdots \; m_K!}
\]
is the multinomial coefficient.
\end{defi}

\begin{defi}[Dirichlet distribution]
The Dirichlet is a multivariate distribution over $K$ 
random variables $0 \leq \mu_k \leq 1$,
where $k = 1, \dots, K$, subject to the constraints
\[
0 \leq \mu_k \leq 1 \quad 
\text{ and } \quad \sum_{k=1}^K \mu_k = 1.
\]
Denoting $\mu = (\mu_1, \dots, \mu_K)$ and 
$\alpha = (\alpha_1, \dots, \alpha_K)$,
the PDF of Dirichlet distribution is given by 
\[
\dirichlet(\mu \mid \alpha)
= C(\alpha) \prod_{k=1}^K \mu_k^{\alpha_k - 1},
\]
where $C(\alpha)$ is a normalizing constant given by 
\[
C(\alpha) = \frac{\Gamma(\alpha_1 + \dots + \alpha_K)}
{\Gamma(\alpha_1) \dots \Gamma(\alpha_K)}.
\]
\end{defi}

It is easy to verify that the Dirichlet distribution
is the \textbf{conjugate prior} of the multinomial distribution. 
Indeed, 
\[
p(\mu \mid \dcal, \alpha)
\propto p(\dcal \mid \mu) p(\mu \mid \alpha) 
\propto \prod_{k=1}^K \mu_k^{\alpha_k + m_k - 1},
\]
so $p(\mu \mid \dcal, \alpha)
= \dirichlet(\mu \mid \alpha + m)$, where we denote 
$m = (m_1, \dots, m_K)$.

\subsection{The Gaussian distribution}

\begin{defi}[Gaussian distribution]
The $D$-dimensional multivariate 
\textbf{Gaussian distribution} is given by 
\[
\ncal (x \mid \mu, \Sigma) 
= \frac{1}{(2\pi)^{D / 2} \abs{\Sigma}^{1 / 2}}
\exp \left[ -\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu) 
\right],
\]
where $\mu$ is the \textbf{mean} and $\Sigma$ is the 
\textbf{covariance matrix}.
The inverse of the covariance matrix $\Lambda = \Sigma^{-1}$ 
is called the \textbf{precision matrix}.
\end{defi}

The moment and covariance of the Gaussian distribution
is given by 
\[
\E[x] = \mu \quad \text{ and } \quad 
\E[x x^T] = \mu \mu^T + \Sigma.
\]
Therefore, the covariance (or variance) is 
\[
\cov[x] = \E \left[ (x - \E[x]) (x - \E[x])^T \right]
= \Sigma.
\]

Note that the functional dependence of the Gaussian on 
$x$ is through the quadratic from
\[
\Delta^2 = (x - \mu)^T \Sigma^{-1} (x - \mu)
\]
which appears in the exponent. It is usually the case 
that we can identify some distribution is Gaussian by 
noting the quadratic form in the exponent. We can then 
identify the mean and variance of the distribution by 
\textbf{completing the square}. When completing the square,
the following identity is very helpful:
\[
- \frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu) 
= - \frac{1}{2} x^T \Sigma^{-1} x 
+ x^T \Sigma^{-1} \mu + \const,
\]
where the constant is independent of $x$. This can helps 
us read off the precision matrix and the mean from quadratic
term and linear term of the expression.

\subsubsection{Conditionals and marginals of Gaussian 
distribution}

When considering the conditional and marginals of Gaussian
distributions, we often need to deal with \textbf{partitioned
matrix}. The inverse of a partitioned matrix and itself 
is related through the following identity: 
\[
\begin{bmatrix}
  A & B \\
  C & D
\end{bmatrix}^{-1} 
= 
\begin{bmatrix}
  M & - M B D^{-1} \\
  - D^{-1} C M & D^{-1} + D^{-1} C M B D^{-1}
\end{bmatrix},
\]
where 
\[
M = (A - B D^{-1} C)^{-1}.
\]

\begin{thm}
Given a joint Gaussian distribution $\ncal (x \mid \mu,
\Sigma)$ with $\Lambda = \Sigma^{-1}$, and 
\[
x = \begin{bmatrix}
  x_a \\
  x_b
\end{bmatrix}, 
\quad 
\mu = \begin{bmatrix}
  \mu_a \\
  \mu_b
\end{bmatrix}, 
\quad 
\Sigma = \begin{bmatrix}
  \Sigma_{aa} & \Sigma_{ab} \\
  \Sigma_{ba} & \Sigma_{bb}
\end{bmatrix}, 
\quad 
\Lambda = \begin{bmatrix}
  \Lambda_{aa} & \Lambda_{ab} \\
  \Lambda_{ba} & \Lambda_{bb}
\end{bmatrix}.
\]
We have the conditional distribution: 
\[
p(x_a \mid x_b) 
= \ncal (x \mid \mu_{a \mid b}, \Lambda_{aa}^{-1}),
\]
where 
\[
\mu_{a \mid b} = \mu_a - \Lambda_{aa}^{-1} \Lambda_{ab} 
(x_b - \mu_b),
\]
and the marginal distribution 
\[
p(x_a) = \ncal(x_a \mid \mu_a, \Sigma_{aa}).
\] 
\end{thm}

\subsubsection{Bayes' theorem for Gaussian distribution}

\begin{thm}
Given a marginal Gaussian distribution for $x$ and a 
conditional Gaussian distribution for $y$ given $x$ in 
the form
\[
\begin{aligned}
p(x) &= \ncal (x \mid \mu, \Lambda^{-1}), \\
p(y \mid x) &= \ncal (y \mid A x + b, L^{-1}),
\end{aligned}
\]
the marginal distribution of $y$ and the conditional 
distribution of $x$ given $y$ are given by 
\[
\begin{aligned}
p(y) &= \ncal (y \mid A \mu + b, L^{-1} + A \Lambda^{-1} A), \\
p(x \mid y) &= \ncal (x \mid \Sigma (A^T L (y - b) + 
\Lambda \mu), \Sigma),
\end{aligned}
\]
where 
\[
\Sigma = (\Lambda + A^T L A)^{-1}.
\]
\end{thm}

\subsubsection{Maximum likelihood for the Gaussian}

Given a data set $X = \{x_1, \dots, x_N\}$ of observations 
drawn independent from a Gaussian distribution, we can 
estimate the parameters of the distribution by maximum 
likelihood. The log likelihood function is given by
\[
\log p(X \mid \mu, \Sigma)
= -\frac{N D}{2} \log (2 \pi) - \frac{N}{2} \log \abs{\Sigma}
- \frac{1}{2} \sum_{n=1}^N (x_n - \mu)^T \Sigma^{-1} 
(x_n - \mu).
\]
It is easy to notice that this only depends on $\sum_{n=1}^N 
x_n$ and $\sum_{n=1}^N x_n x_n^T$. These quantities are 
called the \textbf{sufficient statistics} for the Gaussian 
distribution. We can easily 
derive the maximum likelihood estimation by setting the
corresponding partial derivative to zero: 
\[
\mu_{\ML} = \frac{1}{N} \sum_{n=1}^N x_n, 
\quad 
\Sigma_{\ML} = \frac{1}{N} \sum_{n=1}^N 
(x_n - \mu_{\ML}) (x_n - \mu_{\ML})^T.
\]

Recall the moment of the Gaussian distribution 
$\E[x] = \mu$ and $\E[x x^T] = \mu \mu^T + \Sigma$.
It follows that 
\[
\begin{aligned}
\E[x_n] = \mu, 
\quad 
\E[x_n \mu_{\ML}^T] 
= \frac{N - 1}{N} \mu \mu^T + \frac{1}{N} \Sigma
\end{aligned}
\]
for any $1 \leq n \leq N$, and 
\[
\E[\mu_{\ML} \mu_{\ML}^T]
= \frac{N^2 - 1}{N^2} \mu \mu^T 
+ \frac{N}{N^2 - 1} \Sigma.
\]
Therefore, if we evaluate the maximum likelihood estimation 
under the true distribution, we obtain
\[
\E[\mu_{\ML}] = \mu,
\quad 
\E[\Sigma_{\ML}] = \frac{N - 1}{N} \Sigma,
\]
We can see that the estimation for mean is unbiased but the 
estimation for variance is biased. We can correct this 
by 
\[
\tilde{\Sigma} = \frac{1}{N - 1} \sum_{n=1}^N 
(x_n - \mu_{\ML}) (x_n - \mu_{\ML})^T.
\]
It is clear that now 
\[
\E[\tilde{\Sigma}] = \Sigma,
\]
and $\tilde{\Sigma}$ is called the \textbf{unbiased 
estimator} for the Gaussian distribution.

\subsubsection{Bayesian inference for the Gaussian}

Now consider a Bayesian model for Gaussian distribution.
Suppose first the variance $\sigma^2$ of a Gaussian
distribution is known and we 
want to infer the mean $\mu$ with observed data 
$X = \{x_1, \dots, x_N\}$. The likelihood function is 
given by 
\[
p(X \mid \mu) = \frac{1}{(2 \pi \sigma^2)^{N / 2}}
\exp \left[ - \frac{1}{2 \sigma^2} 
\sum_{n=1}^N (x_n - \mu)^2 \right].
\]
It is important to note that this is not a probability
distribution over $\mu$.
If we take the prior to be also Gaussian, then it will be 
conjugate to this likelihood function. Therefore, 
we take our prior distribution to be 
\[
p(\mu) = \ncal(\mu \mid \mu_0, \sigma_0^2).
\]
Then the posterior is given by $p(\mu \mid X) 
\propto p(X \mid \mu) p(\mu)$. Through calculation, 
we can obtain that $p(\mu \mid X) = \ncal(\mu \mid
\mu_N, \sigma_N)$, where 
\[
\begin{aligned}
  \mu_N &= \frac{\sigma^2}{N \sigma_0^2 + \sigma^2} 
  \mu_0 + \frac{N \sigma_0^2}{N \sigma_0^2 + \sigma^2} 
  \mu_{\ML}, \\
  \frac{1}{\sigma_N^2} &= \frac{1}{\sigma_0^2} 
  + \frac{N}{\sigma^2},
\end{aligned}
\]
in which $\mu_{\ML}$ is the maximum likelihood estimation 
given by 
\[
\mu_{\ML} = \frac{1}{N} \sum_{n=1}^N x_n.
\]
Note that as we increase the number of observed data points (taking the 
limit as $N \to \infty$), the precision steadily increases, 
corresponding to a posterior distribution with steadily 
decreasing variance.

Now suppose the mean $\mu$ is known and we hope to infer 
the variance. It turns out working with the precision 
$\lambda = \frac{1}{\sigma^2}$ is more convenient. We 
again write down the likelihood function: 
\[
p(X \mid \lambda) 
\propto \lambda^{N / 2} 
\exp \left[ - \frac{\lambda}{2} \sum_{n=1}^N 
(x_n - \mu)^2 \right].
\]
To choose a conjugate prior, we introduce the 
gamma distribution.

\begin{defi}[gamma distribution]
The \textbf{gamma distribution} is given by 
\[
\gammaD(\lambda \mid a, b) = 
\frac{1}{\Gamma(a)} b^a \lambda^{a - 1} \exp (-b \lambda).
\]  
In particular, we have the following integral: 
\[
\begin{aligned}
\int_0^\infty \lambda^{a - 1} \exp(- b \lambda) \d \lambda
= \int_0^\infty \frac{1}{b^a} u^{a - 1} e^{-u} \d u 
= \frac{1}{b^a} \Gamma(a),
\end{aligned}
\]
where we used change of variable $u = b \lambda$
in the first equality.
\end{defi}

Through more gamma function calculation, we can 
obtain the mean and variance of the gamma distribution:
\[
\E[\lambda] = \frac{a}{b}, 
\quad 
\var[\lambda] = \frac{a}{b^2}.
\]

Now consider a prior distribution $\gammaD(\lambda 
\mid a_0, b_0)$. We can then obtain the posterior
\[
p(\lambda \mid X) 
\propto \lambda^{a_0 - 1} \lambda^{N / 2} 
\exp \left[ - b_0 \lambda - \frac{\lambda}{2} 
\sum_{n=1}^N (x_n - \mu)^2 \right],
\]
which is easily identified as a gamma distribution with 
\[
\begin{aligned}
a_N &= a_0 + \frac{N}{2}, \\
b_N &= b_0 + \frac{1}{2} \sum_{n=1}^N (x_n - \mu)^2
= b_0 + \frac{N}{2} \sigma_{\ML}^2,
\end{aligned}
\]
and the normalization constant then follows easily.

Now suppose both the mean and the variance is unknown.
Consider the likelihood function: 
\[
\begin{aligned}
p(X \mid \mu, \lambda)
&= \prodn^N \frac{\lambda^{1 / 2}}{(2 \pi)^{1 / 2}}
\exp \left[ - \frac{\lambda}{2} (x_n - \mu)^2 \right] \\
&\propto \left[ \lambda^{1 / 2} \exp \left( - 
\frac{\lambda \mu^2}{2} \right) \right]^{N}
\exp \left[ \lambda \mu \sumn^N x_n 
- \frac{\lambda}{2} \sumn^N x_n^2 \right].
\end{aligned}
\]
Therefore, we should be looking for a conjugate prior 
in the form of 
\[
\begin{aligned}
p(\mu, \lambda)
&\propto \left[ \lambda^{1 / 2} \exp \left( - \frac{\lambda \mu^2}
{2} \right) \right]^\beta \exp \left( c \lambda \mu - 
d \lambda \right) \\
&= \exp \left[ - \frac{\lambda \beta}{2} 
\left( \mu - \frac{c}{\beta} \right)^2 \right]
\lambda^{\beta / 2} \exp \left[ - \left( d - 
\frac{c^2}{2 \beta} \right) \lambda \right].
\end{aligned}
\]
This is the \textbf{Gaussian-gamma distribution}: 
\[
p(\mu, \lambda) 
= \ncal (\mu \mid \mu_0, (\beta \lambda)^{-1}) 
\gammaD (\lambda \mid a, b),
\]
where $\mu_0 = \frac{c}{\beta}$, $a = \frac{\beta}{2} + 1$,
and $b = d - \frac{c^2}{2 \beta}$. 

In the case of a multivariate Gaussian $\ncal (x \mid \mu, 
\Lambda^{-1})$ for a $D$-dimensional variable $x$, 
the conjugate prior for the mean is again Gaussian
if we assume the variance is known. If the mean is 
known and the variance is unknown, the conjugate prior
is a \textbf{Wishart distribution} given by 
\[
\wcal (\Lambda \mid W, \nu)
= B \abs{\Lambda}^{(\nu - D - 1) / 2} 
\exp \left[ -\frac{1}{2} \tr (W^{-1} \Lambda) \right],
\]
where $\nu$ is the degrees of freedom and $W$ is a 
$D \times D$ scale matrix. The normalization constant 
$B$ is given by 
\[
B(W, \nu) = \abs{W}^{- \nu / 2} 
\left[ 2^{\nu D / 2} \pi^{D (D - 1) / 4} 
\prodi^D \Gamma \left( \frac{\nu + 1 - i}{2} \right) 
\right]^{-1}.
\]
For unknown mean and unknown precision, the conjugate
prior is the \textbf{Gaussian-Wishart distribution} 
given by 
\[
p(\mu, \Lambda \mid \mu_0, \beta, W, \nu) 
= \ncal (\mu \mid \mu_0, (\beta \Lambda)^{-1}) 
\wcal (\Lambda \mid W, \nu).
\]

\subsubsection{Student's t-distribution}

We have seen the conjugate prior for the precision 
of a Gaussian is given by the gamma distribution. Now 
suppose we have a univariate Gaussian $\ncal(x \mid \mu, 
\tau^{-1})$ together with a gamma prior $\gammaD (\tau \mid 
a, b)$, we integrate over the precision to get
\[
\begin{aligned}
p(x \mid \mu, a, b) 
&= \int_0^\infty \ncal(x \mid \mu, \tau^{-1}) 
\gammaD(\tau \mid a, b) \d \tau \\
&= \int_0^\infty \frac{\tau^{1 / 2}}{(2 \pi)^{1 / 2}} 
\exp \left[ -\frac{\tau}{2} (x - \mu)^2 \right] 
\frac{1}{\Gamma(a)} b^a \tau^{a - 1} \exp (- \tau b) \d \tau \\
&= \frac{b^a}{\Gamma(a)} \frac{1}{(2 \pi)^{1 / 2}} 
\left[ b + \frac{(x - \mu)^2}{2} + b \right]^{- a - 1 / 2}
\Gamma \left( a + 1 / 2 \right).
\end{aligned}
\]
Now we introduce a new distribution.
\begin{defi}[student's t-distribution]
The \textbf{student's t-distribution} is given by 
\[
\St (x \mid \mu, \lambda, \nu)
= \frac{\Gamma \left( \nu / 2 + 1 / 2 \right)}
{\Gamma \left( \nu / 2 \right)} 
\left( \frac{\lambda}{\pi \nu} \right)^{1 / 2} 
\left[ 1 + \frac{\lambda (x - \mu)^2}{\nu} \right]^{ - 
\nu / 2 - 1 / 2},
\]
where $\nu$ is called the \textbf{degrees of freedom}
of the distribution.
\end{defi}

Student's t-distribution have longer tails than the 
Gaussian distribution, so it is more \textbf{robust} 
-- much less sensitive to a few outliers.

It is easy to verify that 
\[
\St(x \mid \mu, \lambda, \nu) 
= \int_0^\infty \ncal(x \mid \mu, (\eta \lambda)^{-1})
\gammaD (\eta \mid \nu / 2, \nu / 2) \d \eta.
\]
We can then generalize this to \textbf{a multivariate 
student's t-distribution}: 
\[
\St (x \mid \mu, \Lambda, \nu) 
= \int_0^\infty \ncal(x \mid \mu, (\eta \Lambda)^{-1}) 
\gammaD(\eta \mid \nu / 2, \nu / 2) \d \eta.
\]
Through computation, we obtain 
\[
\St (x \mid \mu, \Lambda, \nu) 
= \frac{\Gamma(D / 2 + \nu / 2)}{\Gamma(\nu / 2)} 
\frac{\abs{\Lambda}^{1 / 2}}{(\pi \nu)^{D / 2}}
\left( 1 + \frac{\Delta^2}{\nu} \right)^{- D / 2 - 
\nu / 2},
\]
where $D$ is the dimensionality of $x$ and 
\[
\Delta^2 = (x - \mu)^T \Lambda (x - \mu).
\]
We also have 
\[
\begin{aligned}
  \E[x] &= \mu & \text{if $\nu > 1$}, \\
  \cov[x] &= \frac{\nu}{(\nu - 2)} \Lambda^{-1} 
  & \text{if $\nu > 2$}, \\
  \mode[x] &= \mu.
\end{aligned}
\]

\subsection{The exponential family}

\begin{defi}
The exponential family of distributions over $x$, 
given parameters $\eta$, is defined to be the set of 
distributions of the form
\[
p(x \mid \eta) = h(x) g(\eta) \exp \left( \eta^T u(x) 
\right).
\]
Here $\eta$ is called the \textbf{natural parameters}
of the distribution, $u(x)$ is some function of $x$ 
and $g(\eta)$ can be interpreted as a normalization 
constant so that 
\[
g(\eta) \int h(x) \exp \left( \eta^T u(x) \right) \d x = 1.
\]
\end{defi}

For any member in the exponential family, there is a 
\textbf{conjugate prior} that can be written in the form of 
\[
p(\eta \mid \chi, \nu)
= f(\chi, \nu) g(\eta)^\nu \exp \left( \nu \eta^T \chi 
\right),
\]
where $f(\chi, \nu)$ is a normalization constant, and 
$g$ is the same function as in the likelihood distribution
$p(x \mid \eta)$. To see this is indeed a conjugate prior,
note that we have 
\[
p(\eta \mid X, \chi, \nu) 
\propto g(\eta)^{\nu + N} \exp \left[ \eta^T 
\left( \nu \chi + \sum_{n=1}^N u(x_n) \right) \right].
\]
This is indeed in the same functional form as the 
prior distribution $p(\eta \mid \chi, \nu)$.
Moreover, $\nu$ can be interpreted as a effective number of
pseudo-observations in the prior, each of which has a value 
for the sufficient statistic $u(x)$ given by $\chi$.





\section{Mixture models and EM}

\subsection{Kullback-Leibler (KL) Divergence}
In this section, we adopt the convention that $0 \log 0 = 0$.

\begin{defi}[KL divergence]
  The \textbf{Kullback-Leibler (KL) divergence} of two
  distributions $P(X)$ and $Q(X)$
  over the outcome space $X$ is defined as follows:
  \[
  \KL (P \; \Vert \; Q) = \sum_{x \in X} P(x) \log \frac{P(x)}{Q(x)}.
  \]
\end{defi}

To understand the significance of KL-divergence better,
we first discuss some related concepts in information theory,
starting with the definition of \textbf{entropy}.
\begin{defi}
  The \textbf{entropy} of a distribution $P(X)$
  is defined as
  \[
  H(P) = - \sum_{x \in X} P(x) \log P(x)
  \]
\end{defi}

Intuitively, entropy measures how dispersed a probability
distribution is. For example, a uniform distribution is
considered to have very high entropy (i.e. a lot of uncertainty),
whereas a distribution that assigns all its mass on a single
point is considered to have zero entropy (i.e. no uncertainty).
Notably, it can be shown that among continuous distributions
over $\R$, the Gaussian distribution $\ncal(\mu, \sigma^2)$ has
the highest entropy (highest uncertainty) among all possible
distributions that have the given mean $\mu$ and variance $\sigma^2$.

To further solidify our intuition, we present motivation from
communication theory. Suppose we want to communicate from a
source to a destination, and our messages are always
(a sequence of) discrete symbols over space $X$
(for example, $X$ could be letters \{a, b, \dots, z\}).
We want to construct an encoding scheme for our symbols
in the form of sequences of binary bits that are transmitted
over the channel. Further, suppose that in the long run the
frequency of occurrence of symbols follow a probability
distribution $P(X)$. This means, in the long run, the fraction
of times the symbol $x$ gets transmitted is $P(x)$.

A common desire is to construct an encoding scheme such that
the average number of bits per symbol transmitted remains as
small as possible. Intuitively, this means we want very
frequent symbols to be assigned to a bit pattern having a
small number of bits. Likewise, because we are interested in
reducing the average number of bits per symbol in the long
term, it is tolerable for infrequent words to be assigned to
bit patterns having a large number of bits, since their low
frequency has little effect on the long term average. The
encoding scheme can be as complex as we desire, for example,
a single bit could possibly represent a long sequence of
multiple symbols (if that specific pattern of symbols is very
common). The entropy of a probability distribution $P(X)$ is
its optimal bit rate, i.e., the lowest average bits per
message that can possibly be achieved if the symbols $x \in X$
occur according to $P(X)$. It does not specifically tell us
how to construct that optimal encoding scheme. It only tells
us that no encoding can possibly give us a lower long term
bits per message than $H(P)$.

To see a concrete example, suppose our messages have a
vocabulary of $K = 32$ symbols, and each symbol has an equal
probability of transmission in the long term (i.e, uniform
probability distribution). An encoding scheme that would
work well for this scenario would be to have $\log_2 K$ bits
per symbol, and assign each symbol some unique combination
of the $\log_2 K$ bits. In fact, it turns out that this is the
most efficient encoding one can come up with for the uniform
distribution scenario.

It may have occurred to you by now that the long term average
number of bits per message depends only on the frequency of
occurrence of symbols. The encoding scheme of scenario A can
in theory be reused in scenario B with a different set of
symbols (assume equal vocabulary size for simplicity),
with the same long term efficiency, as long as the symbols
of scenario B follow the same probability distribution as the
symbols of scenario A. It might also have occured to you,
that reusing the encoding scheme designed to be optimal for
scenario A, for messages in scenario B having a different
probability of symbols, will always be suboptimal for
scenario B. To be clear, we do not need know what the
specific optimal schemes are in either scenarios. As long
as we know the distributions of their symbols, we can say
that the optimal scheme designed for scenario A will be
suboptimal for scenario B if the distributions are different.

Concretely, if we reuse the optimal scheme designed for a
scenario having symbol distribution $Q(X)$, into a scenario
that has symbol distribution $P(X)$, the long term average
number of bits per symbol achieved is called the cross entropy,
denoted by $H(P,Q)$:

\begin{defi}
  The cross-entropy of two distributions $P(X)$
  and $Q(X)$ is defined as
  \[
  H(P, Q) = - \sum_{x \in X} P(X) \log Q(x)
  \]
\end{defi}

To recap, the entropy $H(P)$ is the best possible long term
average bits per message (optimal) that can be achived under
a symbol distribution $P(X)$ by using an encoding scheme
(possibly unknown) specifically designed for $P(X)$.
The cross entropy $H(P,Q)$ is the long term average bits
per message (suboptimal) that results under a symbol
distribution $P(X)$, by reusing an encoding scheme
(possibly unknown) designed to be optimal for a scenario
with symbol distribution $Q(X)$.

Now, KL divergence is the penalty we pay, as measured in
average number of bits, for using the optimal scheme for
$Q(X)$, under the scenario where symbols are actually
distributed as $P(X)$. It is straightforward to see this:
\[
\begin{aligned}
  \KL(P \; \Vert \; Q)
  &= \sum_{x \in X} P(x) \log \frac{P(x)}{Q(x)} \\
  &= \sum_{x \in X} P(x) \log P(x) - \sum_{x \in X}
  P(x) \log Q(x) \\
  &= H(P, Q) - H(P).
  \quad \text{ (difference in average number of bits) }
\end{aligned}
\]

If the cross entropy between $P$ and $Q$ is zero
(and hence $\KL(P \; \Vert \; Q) = 0$) then it necessarily
means $P = Q$. In Machine Learning, it is a common
task to find a distribution $Q$ that is ``close'' to
another distribution $P$. To achieve this, we use
$\KL(P \; \Vert \; Q)$ to be the loss function to be optimized.
As we will see in this below, Maximum Likelihood
Estimation, which is a commonly used optimization objective,
turns out to be equivalent minimizing KL divergence between
the training data (i.e. the empirical distribution over the
data) and the model.

Now we present some useful properties of KL divergence.

\begin{thm}[Non-negativity]
  For any distribution $P$ and $Q$, we have
  \[
  \KL(P \; \Vert \; Q) \geq 0,
  \]
  and $\KL(P \; \Vert \; Q) = 0$ if and only if $P = Q$ (almost
  everywhere).
\end{thm}

\begin{proof}
  By definition,
  \[
    \KL(P \; \Vert \; Q) = \sum_{x \in X} P(x) \log \frac{P(x)}{Q(x)} = - \sum_{x \in X} P(x) \log \frac{Q(x)}{P(x)}.
  \]
  Since $-\log x$ is strictly convex, by Jensen's inequality, we have
  \[
  \begin{aligned}
    \KL(P \; \Vert \; Q) = - \sum_{x \in X} P(x) \log \frac{Q(x)}{P(x)}
    \geq -\log \sum_{x \in X} P(x) \frac{Q(x)}{P(x)} = 0.
  \end{aligned}
  \]
  When the equality holds,
  \[
  \log \frac{Q(x)}{P(x)} = 0
  \]
  almost everywhere.
  That is, $Q = P$ almost everywhere.
  This completes the proof.
\end{proof}

\begin{defi}[KL divergence of conditional distribution]
  The KL divergence between 2 conditional distributions
  $P(X \mid Y)$, $Q(X \mid Y)$ is defined as follows:
  \[
  \KL({P(X \mid Y)} \; \Vert \; {Q(X \mid Y)}) = \sum_y P(y)
  \left( \sum_x P(x \mid y) \log
  \frac{P(x \mid y)}{Q(x \mid y)} \right).
  \]
  This can be thought of as the expected KL divergence
  between the corresponding conditional distributions on
  $x$. That is, between $P(X \mid Y = y)$ and $Q(X \mid Y = y)$,
  where the expectation is taken over the random $y$.
\end{defi}

\begin{thm}[Chain rule for KL divergence]
The following equality holds:
\[
\KL({P(X, Y)} \; \Vert \; {Q(X, Y)}) =
\KL({P(X)} \; \Vert \; {Q(X)}) + \KL({P(Y \mid X)}
\; \Vert \; {Q(Y \mid X)}).
\]
\end{thm}

\begin{proof}
\[
\begin{aligned}
  \text{LHS} &= \sum_x \sum_y P(x,y) \log \frac{P(x,y)}{Q(x,y)} \\
  &= \sum_x \sum_y P(y \mid x) P(x) \left[ \log
  \frac{P(y \mid x)}{Q(y \mid x)} + \log \frac{P(x)}{Q(x)} \right] \\
  &= \sum_x \sum_y P(y \mid x) P(x) \log
  \frac{P(y \mid x)}{Q(y \mid x)} + \sum_x P(x) \log
  \frac{P(x)}{Q(x)} \sum_y P(y \mid x) \\
  &= \sum_x \sum_y P(y \mid x) P(x) \log \frac{P(y \mid x)}
  {Q(y \mid x)} + \sum_x P(x) \log \frac{P(x)}{Q(x)} \\
  &= \KL({P(X)} \; \Vert \; {Q(X)}) + \KL({P(Y \mid X)} \; \Vert \; {Q(Y \mid X)}) \\
  &= \text{RHS}.
\end{aligned}
\]
\end{proof}

\subsection{The EM Algorithm in General}

Consider a probabilistic model in which we collectively
denote all of the observed variables by $X$ and all of
the hidden variables by $Z$. The joint distribution
$p(X, Z \mid \theta)$ is governed by a set of parameters
denoted $\theta$. Our goal is to maximize the likelihood
function that is given by
\[
p(X \mid \theta) = \sum_Z p(X, Z \mid \theta).
\]

Assume direct optimiaztion of $p(X \mid \theta)$ is difficult
but optimiaztion of $p(X, Z \mid \theta)$ is easy.
Introduce distribution $q(Z)$ for the latent vairable $Z$,
we then have
\[
\log p(X \mid \theta) = \L(q, \theta) + \KLD{q}{p},
\]
where
\[
\begin{aligned}
  \L(q, \theta) &= \sum_Z q(Z) \log \frac{p(X, Z \mid \theta)}{q(Z)}, \\
  \KLD{q}{p} &= - \sum_Z q(Z) \log \frac{p(Z \mid X, \theta)}{q(Z)}.
\end{aligned}
\]
Note that $\L$ is a functional of the distribution $q(Z)$.
Note also that $\KLD{q}{p}$ is the KL divergence of $q(Z)$
and $p(Z \mid X, \theta)$ so $\KLD{q}{p} \geq 0$
Therefore, $\L(q, \theta) \leq \log p(X \mid \theta)$. In other word,
$\L(q, \theta)$ is a \textbf{lower bound}
for $\log p(X \mid \theta)$.

Therefore, for the EM algorithm, suppose currently the model
parameter is $\theta_t$. In the E step, the lower bound
$\L(q, \theta)$ is maximized with respect to $q(Z)$ while
holding $\theta$ fixed. Note that $\log p(X \mid \theta_t)$
is not dependent on $q(Z)$, so the lower bound will
be maximized when $\KLD{q}{p} = 0$. This is equivalent to
when $q(Z) = p(Z \mid X, \theta)$. Therefore, in the E
step we simply set
\[
q(Z) = p(Z \mid X, \theta_t).
\]

In the subsequent M step, the distribution $q(Z)$ is fixed
and the lower bound $\L(q, \theta)$ is maximized with
respect to $\theta$ to give some new model parameter
$\theta_{t + 1}$. Because the distribution $q$ is determined
using the old parameter values rather than the new values
and is held fixed during the M step, it will not equal the
new posterior distribution $p(Z \mid X, \theta_{t+1})$,
and hence there will be a nonzero KL divergence.
Substituting the result from E step, we have the following
expression for the lower bound of M step:
\[
\begin{aligned}
  \L(q, \theta)
  &= \sum_Z p(Z \mid X, \theta_t) \log p(X, Z \mid \theta)
  - \sum_Z p(Z \mid X, \theta_t) \log p(Z \mid X, \theta_t) \\
  &= \Q(\theta_t, \theta) + \const,
\end{aligned}
\]
where
\[
\Q(\theta_t, \theta) = \sum_Z p(Z \mid X, \theta_t)
\log (X, Z \mid \theta)
\]
and the constant is simply the negative entropy of
the $q$ distribution. Note that the variable $\theta$
over which we are optimizing appears only inside the
logarithm.

The EM algorithm can also be used to maximize the
\textbf{posterior}
$p(\theta \mid X)$ with a prior $p(\theta)$
of the parameters. This is done by noting that
\[
\begin{aligned}
\log p(\theta \mid X)
&= \log p(\theta, X) - \log (X) \\
&= \L(q, \theta) + \KLD{q}{p} + \log p(\theta) - \log p(X) \\
&\geq \L(q, \theta) + \log p(\theta) - \log p(X),
\end{aligned}
\]
where $\log p(X)$ is a constant.


\section{Approximate inference}
A central task in the application of probabilistic models is
the evaluation of the posterior distribution $p(Z \mid X)$ of the
latent variables $Z$ given the observed (visible) data variables
$X$, and the evaluation of expectations computed with respect
to this distribution. For many models of practical interest,
it will be infeasible
to evaluate the posterior distribution or indeed to compute
expectations with respect to this distribution.
This could be because the dimensionality of
the latent space is too high to work with directly or
because the posterior distribution has a highly complex
form for which expectations are not analytically tractable.
In the case of continuous variables, the required
integrations may not have closed-form analytical solutions,
while the dimensionality of the space and the complexity of
the integrand may prohibit numerical integration.
For discrete variables, the marginalizations involve
summing over all possible configurations of the hidden
variables, and though this is always possible in principle,
we often find in practice that there may be exponentially
many hidden states so that exact calculation is
prohibitively expensive. The following table summarizes 
the properties of the mentioned distributions:

\begin{center}
\begin{tabular}{lccl}
Concept & Known? & Tractable? & Notes \\
\hline
$p(Z)$ & \cmark & \cmark & Chosen prior distribution \\
$p(X \mid Z)$ & \cmark & \cmark & Likelihood model 
(possibly NN) \\ 
$p(X, Z)$ & \cmark & \cmark & Product of above two \\ 
$p(Z \mid X)$ & \xmark & \xmark & Intractable posterior \\
$p(X)$ & \xmark & \xmark & Intractable marginals \\
\end{tabular}
\end{center}

In this chapter, we introduce a range of deterministic
approximation schemes, some of which scale well to large
applications. These are based on analytical approximations
to the posterior distribution, for example by assuming that
it factorizes in a particular way or that it has a specific
parametric form such as a Gaussian. As such, they can never
generate exact results, and so their strengths and weaknesses
are complementary to those of sampling methods.

\subsection{Variational inference}

Suppose we have a fully Bayesian model in which every parameter
is given prior distributions. Denote all latent variable with
$Z$ and all observed variable with $X$. As before we have
\[
\log p(X) = \L(q) + \KLD{q}{p},
\]
where
\[
\begin{aligned}
  \L(q) &= \int q(Z) \log \frac{p(X, Z)}{q(Z)} \d Z, \\
  \KLD{q}{p} &= - \int q(Z) \log \frac{p(Z \mid X)}{q(Z)}
  \d Z.
\end{aligned}
\]
It is well known that the KL divergence $\KLD{q}{p}$ is
maximized when $q(Z) = p(Z \mid X)$. However, here we
assume the posterior $p(Z \mid X)$ is intractable.
We therefore consider instead a restricted family of
distributions $q(Z)$ and then seek the member of this
family for which the KL divergence is minimized.

\subsubsection{Factorized distributions}
Suppose paritioning the latent variables $Z$ into disjoint
groups $Z_1, \dots, Z_m$, and assume the $q$ distribution
factorizes in the form
\[
q(Z) = \prod_{i=1}^M q_i(Z_i).
\]
This factorized form of variational inference corresponds to an
approximation framework developed in physics called \textbf{mean
field theory}.

We know wish to optimize $\L(q)$ with respect to all distributions
$q_i(Z_i)$. We first compute
\[
\begin{aligned}
\L(q)
&= \int \prod_i q_i(Z_i) \left[ \log p(X, Z) - \sum_i
\log q_i(Z_i) \right] \d Z \\
&= \int q_j \left[ \int \log p(X, Z) \prod_{i \neq j}
q_i(Z_i) \d Z_i \right] \d Z_j
- \int q_j \log q_j \d Z_j + \const \\
&= \int q_j \log \tilde{p} (X, Z_j) \d Z_j
- \int q_j \log q_j \d Z_j + \const,
\end{aligned}
\]
where we have defined
\[
\log \tilde{p} (X, Z_j) = \E_{i \neq j}
[\log p(X, Z)] + \const.
\]
Suppose we keep $\{q_{i \neq j}\}$ fixed and maximize
$\L(q)$ with respect to one of the factor
$q_j(Z_j)$. Note that this is equivalent
to minimizing the KL divergence of $q_j(Z_j)$ and $\tilde{p}
(X, Z_j)$. Indeed, we have 
\[
\begin{aligned}
\KLD{q_j (Z_j)}{\tilde{p}(X, Z_j)} 
&= \int q_j \log q_j \d Z_j 
- \int q_j \log \tilde{p} (X, Z_j) \d Z_j. 
\end{aligned}
\]
The minimum is thus achieved at $q_j(Z_j)
= \tilde{p} (X, Z_j)$. Hence, the general solution is
\begin{equation*}
  \log q_j^\star (Z_j) = \E_{i \neq j} [\log p(X, Z)] +
  \const,
  \tag{$*$}
\end{equation*}
and the constant is set by normalizing:
\[
q^\star_j(Z_j)
= \frac{\exp (\E_{i \neq j} [\log p(X, Z)])}
{\int \exp (\E_{i \neq j} [\log p(X, Z)]) \d Z_j}.
\]
The algorithm is then to first initialize each $q_i (Z_i)$
appropriately and cycling through all the factors and
replacing each with a revised estimate given by $(*)$.
Convergence is guaranteed because the bound is convex
with respect to each factor $q_i(Z_i)$.

\subsubsection{Properties of factorized approximations}

Alternatively, consider instead that we have been minimizing 
the reverse KL divergence $\KLD{p}{q}$ with respect to 
factorized distribution $q$. The KL divergence
take the form 
\[
\KLD{p}{q} = - \int p(Z) \left[ \sum_{i=1}^M \log q_i 
(Z_i) \right] \d Z + \const.
\]
To optimize this, consider the Lagrangian 
\[
\begin{aligned}
L[q_j] = - \int p(Z) \log q_j(Z_j) \d Z + 
\lambda \int q_j (Z_j) \d Z_j,
\end{aligned}
\]
where we have kept only the terms dependent on $q_j$. 
Note also
\[
\begin{aligned}
\int p(Z) \left[ \sumi^M \log q_i (Z_i) \right] \d Z 
= \int \log q_j(Z_j) \left[ \int p(Z) \prod_{i \neq j} 
d Z_i \right] d Z_j.
\end{aligned}
\]
It follows that
\[
L[q_j] = -\int \log q_j(Z_j) \left[ \int p(Z) \prod_{i \neq j} 
d Z_i \right] d Z_j + \lambda \int q_j(Z_j) \d Z_j.
\]
Taking the functional derivative, we can then obtain
\[
\lambda = 
- \frac{1}{q_j(Z_j)} \int p(Z) \prod_{i \neq j} d Z_j.
\]
Therefore, 
\[
q_j^\star (Z_j) = \int p(Z) \prod_{i \neq j} \d Z_i = p(Z_j).
\]

To compare the two results, note that we have 
\[
\begin{aligned}
\KLD{q}{p} = - \int q(Z) \log \frac{p(Z)}{q(Z)} \d Z, \\
\KLD{p}{q} = - \int p(Z) \log \frac{q(Z)}{p(Z)} \d Z.
\end{aligned}
\]
Note that for $\KLD{q}{p}$, there is a large positive 
contribution in region where $p(Z)$ is small.
Therefore, minimizing $\KLD{q}{p}$ avoids region 
where $p(Z)$ is near $0$. Conversely, 
$\KLD{p}{q}$ is minimized by having $q(Z)$ nonzero 
in region where $p(Z)$ is nonzero. It follows that 
minimizing $\KLD{q}{p}$ often leads to a more \textbf{compact} 
distribution, while minimizing $\KLD{p}{q}$ tends 
to give an \textbf{average across a large region}
(see PRML page 469, 472).

\subsubsection{Model comparison}

As well as performing inference over the hidden variables 
$Z$, we may also wish to compare a set of candidate models, 
labelled by the index $m$, and having prior probabilities 
$p(m)$. Our goal is then to approximate the posterior 
probabilities $p(m \mid X)$, where X is the observed data.

We must recognize that the posterior over $Z$ must be 
conditioned on m, and so we must consider
$q(Z, m) = q(Z \mid m) q(m)$. We then have the following 
variational distribution:
\[
\log p(X) = \L_m - \sum_m \sum_Z q(Z \mid m) q(m) 
\log \frac{p(Z, m \mid X)}{q(Z \mid m) q(m)},
\]
where $\L_m$ is a lower bound for $\log p(X)$ given 
by 
\[
\L_m = \sum_m \sum_Z q(Z \mid m) q(m) 
\log \frac{p(X, Z, m)}{q(Z \mid m) q(m)}.
\]

Now, if we consider a specific model $M$ and 
assign equal prior $p(m)$ to each model $m$, then 
\[
\L = \sum_Z q(Z \mid M) \log \frac{p(X, Z \mid M)}{q(Z \mid M)}
\]
is a lower bound for $\log p(X \mid M)$. Note that 
we also have
\[
p(M \mid X) \propto p(X \mid M) p(M).
\]
However, $p(m)$ are equal for each $m$. This implies that
we can interprete $\L$ as an approximation to the posterior
$p(M \mid X)$, and we can do model selection using the value 
of $\L$ for each model. This should be contrasted with a
maximum likelihood approproach, which tend to overfit 
models (see PRML page 489-490).


\subsection{Illustration: variational mixture of
Gaussians}

Consider the following model: for each observation
$x_n$, we have a corresponding latent variable
$z_n$, which is a 1-of-$K$ binary vector with elements 
$z_{nk}$ for $k = 1, \dots, K$. Denote $X = \{x_1, \dots, 
x_n\}$ and $Z = \{z_1, \dots, z_n\}$. We have the 
following likelihood model:
\[
\begin{aligned}
p(Z \mid \pi) &= \prodn^N \prodk^K \pi_k^{z_{nk}}, \\
p(X \mid Z, \mu, \Lambda)
&= \prodn^N \prodk^K \ncal(x_n \mid \mu_k, \Lambda_k^{-1})
^{z_{nk}}.
\end{aligned}
\]
Now we choose conjugate priors for each parameter. For 
the mixing coefficient $\pi$, we have
\[
\begin{aligned}
p(\pi) = \dirichlet(\pi \mid \alpha_0)
= C(\alpha_0) \prodk^K \pi_k^{\alpha_0 - 1},
\end{aligned}
\]
where we choose the same parameter $\alpha_0$ for each 
component by symmetry. For the mean $\mu$ and precision
$\Lambda$, we have
\[
\begin{aligned}
p(\mu, \Lambda) 
= p(\mu \mid \Lambda) p(\Lambda) 
= \prodk^K \ncal (\mu_k \mid m_0, (\beta_0 \Lambda_k^{-1}))
\wcal (\Lambda_k \mid W_0, \nu_0),
\end{aligned}
\]
where we often choose $m_0 = 0$ by symmetry.

\TODO{do the calculation}

\subsection{Local variational methods}

In previous sections we tried to approximate the whole
posterior distribution, now we introduce an approach
to approximate an \textbf{individual distribution}, often a 
factor in the much larger model specified by a graph.

Consider the \textbf{convex} function $f(x) = \exp (-x)$. 
We want to approximate this with a simpler function.
Consider the first order Taylor expansion
\[
y(x) = f(\xi) + f'(\xi) (x - \xi).
\]
This is a tangent line.
Since $f$ is convex, we know that $y(x) \leq f(x)$,
with equality when $x = \xi$. For our example
$f(x) = \exp (-x)$, so we have 
\[
y(x) = \exp (- \xi) - \exp (- \xi) (x - \xi).
\]
Let $\lambda = - \exp (- \xi)$, we then have
$\xi = - \log (- \lambda)$ and thus 
\[
y(x, \lambda) = 
\lambda x - \lambda + \lambda \log (- \lambda).
\]
Here different $\lambda$ corresponds to different tangent 
lines, which are all lower bounds of the function. 
That is, $y(x, \lambda) \leq f(x)$. Considering the 
equality condition, we also have 
\[
f(x) = \max_\lambda \left\{ \lambda x - \lambda + \lambda 
\log (- \lambda) \right\}.
\]

We can formulate this approach using the framework 
of \textbf{convex duality}. Suppose the equation 
of the tangent line $\lambda x - g(\lambda)$, where 
the slope is $\lambda$ and intercept $- g(\lambda)$.
Consider moving the line $\lambda x$ up until it touches 
$f(x)$. We then have
\[
g(\lambda) = - \min_x \{f(x) - \lambda x\} 
= \max_x \{f(x) - \lambda x\}.
\]
On the other hand, consider a particular $x$ and 
then adjust $\lambda$ until the tangent line is
tangent to $f(x)$ at a particular point $x$. 
Note that the $y$ value at $x$ is maximized when 
this happens, so we have 
\[
f(x) = \max_\lambda \{\lambda x - g(\lambda)\}.
\]
We now see that $f(x)$ and $g(\lambda)$ plays a dual role.
Our analysis generalizes easily to \textbf{concave functions},
for which we have 
\[
\begin{aligned}
f(x) &= \min_\lambda \{\lambda x - g(\lambda)\}, \\
g(\lambda) &= \max_x \{f(x) - \lambda x\}.
\end{aligned}
\]

As an example, let $f(x) = \exp (-x)$.
Consider $g(\lambda) = \max_x \{f(x) - \lambda x\}$. 
We can see that the maximizing 
value of $x$ is given by $\xi = - \log(- \lambda)$, so 
\[
g(\lambda) = -\lambda + \lambda \log (- \lambda),
\]
as obtained previously.

For another example, we want to find an \textbf{upper 
bound} for the logistic sigmoid function given by
\[
\sigma(x) = \frac{1}{1 + e^{-x}}.
\]
This is neither convex nor concave. However, 
\[
f(x) = \log \sigma(x) = - \log (1 + e^{-x})
\]
is concave. We then have 
\[
g(\lambda) = \min_x \{\lambda x - f(x)\}
= - \log \lambda - (1 - \lambda) \log (1 - \lambda),
\]
where the second equality is obtained by taking derivative 
with respect to $x$. Note also this is in the form of 
a binary entropy. We then have $f(x) \leq \lambda x - g(\lambda)$,
and thus
\[
\sigma(x) \leq \exp (\lambda x - g (\lambda)).
\]

We can also obtain a \textbf{lower bound} on the sigmoid.
First we take the log and decompose it:
\[
\begin{aligned}
\log \sigma(x) 
&= - \log (1 + e^{-x}) \\
&= - \log \left[ e^{-x / 2} (e^{x / 2} + 
e^{-x / 2}) \right] \\
&= \frac{x}{2} - \log (e^{x / 2} + e^{- x / 2}).
\end{aligned}
\]
Note that $f(x) = - \log (e^{x / 2} + e^{- x / 2})$ is 
convex function of the variable $x^2$. We then have 
\[
g(\lambda) = \max_{x^2} \left\{ \lambda x^2 - f \left( 
  \sqrt{x^2} \right) \right\}.
\]
Taking the derivative with respect to $x^2$, we have 
\[
0 = \lambda - \frac{d x}{d x^2} \frac{d}{d x} 
f(x) = \lambda + \frac{1}{4x} \tanh \left( \frac{x}{2} 
\right).
\]
Denote this value of $x$ with $\xi$, then we have 
\[
\lambda (\xi) 
= - \frac{1}{4 \xi} \tanh \left( \frac{\xi}{2} \right) 
= - \frac{1}{2 \xi} \left( \sigma(\xi) - \frac{1}{2} \right).
\]
Using $\xi$ as the variational parameter instead of 
$\lambda$, we have 
\[
g(\lambda) = \lambda(\xi) \xi^2 - f(\xi)
= \lambda(\xi) \xi^2 + \log (e^{\xi / 2} - e^{- \xi / 2}).
\]
It then follows by duality that 
\[
f(x) \geq \lambda x^2 - g(\lambda)
= \lambda(\xi) x^2 - \lambda(\xi) \xi^2 
- \log (e^{\xi / 2} - e^{- \xi / 2}).
\]
Therefore, we can obtain the following lower bound for the 
sigmoid function: 
\[
\sigma(x) \geq \sigma(\xi) 
\exp \left[ \frac{x - \xi}{2} - \lambda(\xi) (x^2 - \xi^2) 
\right].
\]

\subsection{Variational logistic regression}

\TODO{add content}

\section{Sampling methods}


\end{document}